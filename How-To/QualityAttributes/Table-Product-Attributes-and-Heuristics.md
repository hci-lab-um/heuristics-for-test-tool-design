# Product Attributes and the Heuristics that Point To Them

| Attribute       |  Notes | Which heuristics are affected? |
|:----------------|:------|--------------|
|Usability: Operability     | Operability is about how well the tool supports people to move through their workflows in use, once then have learned how to use the tool. Around half of all usability comments were about operability, in particular about tools being superficially easy to learn, or having an attractive interface, but then not supporting long term use through the activities over time. That could be improved by thinking in particular about how maintainability of the tests and test artefacts is needed. Additionally, some participants reported that security blockers meant that the tool looked like it would be useful, but they had no access to it - security blocked operability: <br> *“Trying to identify who had access, and what access they had [. . . ] I know how to do it (have learned earlier) but I am still not sure which of the local groups give access to what...''* <br> Operability is important for the effectiveness and efficiency of how people work, and is strongly supported by the other quality attributes. Workflow design is key to supporting operability.|H07, H08, H09, H10, H11 |
|Usability: Learnability    | Learnability is about how easy it to learn to use the tool, it is a pre-requisite for the tool being operable; if you cannot learn it, you cannot use it. About a quarter of usability comments were about the learnability of tools. Some workshop participants discussed a trade-off with some tools being harder to learn, but then more useful, and other tools being easy to learn, but then less powerful. Some management level participants talked about there being no time for training courses, and needing to acquire tools that needed minimal learning. We also found people had strong - and widely differing views - about the media, methods, and purposes of learning - these are reflected in the heuristics. <br> *“that I can quickly get to testing without having to waste time learning the tool, or how the tool wants me to do it”*| H03, H05, H06, H04|
|Usability: User Goals      | The tool needs to support people to meet their goals. Just under a third of participants talked about how well - or otherwise - their tools supported them to meet their goals: <br> *“(the best solution I have found) is a combo of (Tool 1) and (Tool 2), but neither does exactly what I want. Very frustrating. [. . . ] I don’t want to load each page manually and record the results in a spreadsheet so I can show change over time.”* |H01, H02, H08 |
|Portability      |	Portability was the most frequently mentioned quality attribute apart from usability; just over a quarter of points made were about the installability, adaptability and  of tools. Installation, set-up problems, uninstalling, and being able to change to a new tool were most mentioned by testers participating the surveys and workshops.<br> *“Getting the hang of it [. . . ]  processes [. . . ] found it was no longer supported . . . Aargh!”* *``For some reason, everybody who develops tools prepare YouTube video[s] about how to use the tool but not how to install it.”*| H12 |
|Maintainability  | Maintainability includes modularity, reusability, analysability, modifiability.  For test tools the design needs to take in account supporting the maintainability of the tests and test artefacts, as well as consideration of the maintainability of the tool itself, and scaleability of the tool use. <br> *“running the tests is quite easy . . . The difficult part is maintaining the tests when it grows massively.”*| H11, H10, H08, H09|
|Security	      | Nearly a quarter of the participants had comments about security, access rights and the ability to get to the tool to use it. Security breaches were not mentioned so much; this is all about the aspect of security that provide tool and data access appropriate to the testers' types and levels of authorization, rather than blocking them. <br>One person referred to being *`stuck in limbo'* because they were unable to get access to the tool they were required to use, and in discussing these results with practitioners that problems has been recognized by others.| H08, H10|
|Compatibility	  | Around a quarter of participants reported concerns and requirements around co-existence and interoperability of the tool with other tools, and in multiple shared environments | H08, H11|
|Performance      |	Just under 10% of comments were about performance of the tool. Performance includes time behaviour, resource utilization and capacity. Resource utilization will be of interest for any stakeholders looking at economic or environment risk.|H09, H11 |
|Functionality    | Functionality - including the completeness, correctness, and appropriateness of the tool for its use - was lower on the list of people’s concerns than other attributes. Just under half of the participants mentioned it, but it wasn't what they talked about most.| H08|
|Reliability	  | About 10% of participants had requirements and concerns around reliability of the tools. Reliability includes the tool's availability, fault tolerance, and recoverability, while the maturity of the tool's software contributes to its reliability.|H09, H11|
|Accessibility   | Accessibility was mentioned by a small number of participants in terms of performing accessibility testing with specialist tools. However, a question not raised was the accessibility of test tools themselves. This was strongly discussed by expert accessibility reviewer. There are legal and moral implications around workplaces - including test teams - being open to a diverse group of people. | H04, H10, H11 |
|User interface aesthetics | The aesthetics of the user interface less mentioned, and then mainly as a source of irritation if a tool <br> *``looked good but didn't meet expectations.''* <br> The aesthetics is important to support the operability and accessibility of the tool, but is not as important as accessibility and support for learnability, operability and workflows.| H04|
| Appropriateness | A very small number of participants talked about tools not always being appropriate - for their planned purpose but in new context, for example. Two correspondents used the word *`magical'* to describe management expectations of automation. <br> *“supports your workflow vs forcing you to change. People [are greater than] process/tools”*|     H01, H02, H03, H08, H09, H11                                   |
|Recognizability | A very small number of participants talked about tools not always being recognizable for what their purpose was - the naming of a tool or sometimes where it was obtainable from sometimes masked the availability of a useful tool. For example, useful tools in private repositories:  <br> *"[the tool] was in a private GitHub repository"*                                     | H03, H04, H06, H08, H11|
|User error protection |Some participants talked about their fears when using the tools - of looking foolish, of losing data, of making mistakes. Protecting users against making mistakes increases their Effectiveness. <br> *“it is scary and I always get stuck. I am delaying the inevitable (frowny face).”* | H03, H05, H06 |
