# Case Study 1: Evaluating a Vendor Tool

## 1000 word summary generated by Claude Sonnet 4.5

<details><summary>click for 1000 word summary of case study 1</summary>
  
### Method

This case study evaluated a set of twelve heuristics designed to guide tool evaluation processes in software testing contexts. The study took place between January and April 2024 at a multi-national entertainment and leisure company with web-based services and a large engineering department.

The organization was asked by a commercial vendor to evaluate their updated testing tool, which integrated accessibility checking functionality previously provided by a free open-source plugin. The study participant, a key contact within the organization, agreed to use version 1.0 of the heuristics during this evaluation and document the experience.

The twelve heuristics examined were:
- H01: Why do we need this tool?
- H02: Who will use or be affected by the tool?
- H03: What previous experiences do people bring to the tool?
- H04: What communication needs or preferences do those people have?
- H05: What learning perspectives and goals do those people bring?
- H06: What learning preferences do those people have?
- H07: Where will the tool be used?
- H08: What workflows will the tool be part of?
- H09: What risks are associated with those workflows?
- H10: What work styles are acceptable in those workflows and teams?
- H11: When will the tool be used?
- H12: How long will the tool be used?

The evaluation proceeded through five stages: (1) deciding whether to invest time evaluating the tool, (2) planning and setting up the evaluation, (3) carrying out the tool comparison, (4) additional evaluation based on emerging insights, and (5) feedback to the vendor. Data collection involved diary entries documenting which heuristics were used, their usefulness scores, insights gained, and resulting actions.

The participant conducted meetings with the vendor, internal testers, and the organization's UX director. The heuristics were used primarily by the participant to guide planning and execution rather than being explicitly shared with all meeting attendees.

### Results

The tool evaluation revealed significant findings across all five stages. In Stage 1, four heuristics (H01, H02, H07, H08) were used to determine whether a full evaluation was worthwhile. All received the highest scores (5/5) for usefulness, understandability, and flexibility. The heuristics not used at this stage were judged understandable and flexible but not yet useful.

Stage 2 introduced H03 (previous experiences) to select appropriate team members for the evaluation, specifically identifying testers with experience using similar tools. However, the participant adapted this heuristic, noting that the conversation focused more on existing processes and how the integrated tool would impact those workflows rather than on learning needs.

During Stage 3, the actual tool comparison, the evaluation generated five critical insights. First, the tool's excellent user experience eliminated the need to deeply examine learning-related heuristics (H04, H05, H06). The tool was described as one that "doesn't require learning - just delivers a report" with "UX so beautiful you don't need to think about it."

Second, and more significantly, applying heuristics H07 and H08 about workflow prompted fundamental questions about current processes. The evaluation team discovered that accessibility bugs reported by the plugin were never acted upon, leading them to question whether the tool was being used at the right stage in the development process. This represented a shift from evaluating the tool itself to examining the broader organizational context.

Third, this discovery prompted the team to revisit H01 ("Why are we using it?"), revealing that engineers gave low priority to accessibility issues flagged by the tool. This highlighted a disconnect between who was using the tool and who valued its outputs.

Fourth, the team re-examined H02 ("Who is the tool for?"), recognizing that engineers might not be the most appropriate primary users. This led to expanding the evaluation to include the UX department.

A fifth insight concerned evaluation methodology: the participant observed that testers spoke differently about the tool when the vendor was present versus in private conversations, noting this as an important consideration for future heuristics application.

Stage 4 represented a significant expansion of the evaluation based on these insights. The participant engaged the organization's UX director, using eight heuristics (H01-H04, H07-H08, H10-H12) as a personal checklist without explicitly discussing the study. This meeting revealed a critical organizational gap: the UX department had invested in accessibility training and implemented accessibility in designs, but the engineering department never tested whether designs were correctly implemented. Improvements initiated in one team were lost further down the workflow, representing what the participant described as a "chasm" between departments.

The UX director expressed interest in the tool because it could run accessibility tests seamlessly without requiring engineering team intervention, allowing UX to identify where their requirements weren't implemented and create specific requirements for subsequent development iterations. The discussion also highlighted communication issues around severity ratings and priority assignment for accessibility problems.

Stage 5 involved feedback to the vendor but did not explicitly use the heuristics. However, this stage revealed an important gap in the heuristics framework: financial considerations emerged as critical, with the tool potentially doubling existing dashboard costs. The participant noted that cost became "the make or break" factor, yet financial goals weren't addressed early enough in the evaluation process.

### Impact and Evaluation

The heuristics demonstrated substantial impact on the evaluation process and outcomes. All heuristics consistently scored 5/5 for understandability and flexibility across all stages. Usefulness scores varied by context and stage, with four heuristics proving particularly valuable: H01 (Why?), H02 (Who?), H07 (Where?), and H08 (Workflows?).

The most significant impact was redirection of organizational effort. Rather than simply deciding whether to purchase the new tool, the evaluation uncovered fundamental process and communication problems that needed addressing first. The participant concluded that improving workflows and cross-departmental communication offered greater value than tool acquisition. The organization decided to work with the UX department to provide timely feedback from the existing free tool, allowing UX teams to set severity and priority for accessibility issues.

This decision generated quantifiable financial impact, saving the organization 20,000 Euros by avoiding the tool purchase while simultaneously addressing root causes of their accessibility testing challenges. A subsequent insight from the engineering team suggested building accessibility in earlier from designs rather than waiting for issues to be raised, indicating broader organizational learning triggered by the evaluation process.

Three heuristics (H05, H06, H09) were not overtly used. Learning perspectives and preferences (H05, H06) didn't require deep examination because the tool's excellent UX eliminated learning curve concerns. Risk assessment (H09) wasn't prioritized because tool acquisition was considered low-risk in a low-impact area. However, the participant noted these heuristics remained "at the back of my brain" as background considerations.

The participant found the heuristics natural to use, noting that "the conversation naturally flowed in the same order as the heuristics." The heuristics enabled insights that might not have emerged from conventional tool evaluation approaches focused primarily on feature comparison.

Several areas for improvement were identified. Financial considerations need earlier prominence, particularly for tool evaluations. Return on investment should be explicitly addressed under H01 with distinct prompts for tool designers/vendors versus evaluators/purchasers. The detailed information supporting each heuristic proved valuable, but the repository structure could be enhanced with clearer guidance for different evaluation contexts.

The case study process itself could be strengthened for future applications through more structured introduction to the heuristics, explicit question selection for specific contexts, targeted guidance on accessing detailed information, and interim reviews during longer evaluations. The participant's academic background facilitated case study execution; additional support materials may benefit practitioners without research experience.

An important methodological observation emerged regarding taken-for-granted assumptions. The participant noted that heuristics introducing genuinely new perspectives (H04-H06 on communication and learning) may require more active promotion and guidance, as evaluators may unconsciously default to familiar thinking patterns. The effectiveness of these heuristics may vary across organizational contexts depending on existing assumptions and practices.

The participant expressed interest in applying the heuristics to assess their organization's in-house test automation support, suggesting the framework's potential applicability beyond tool acquisition to broader testing infrastructure evaluation. This indicates practical utility and perceived value from the initial application.

Overall, this case study demonstrates that the heuristics successfully guided a systematic evaluation process, generated insights extending beyond the immediate tool comparison, prompted organizational learning about process gaps and communication barriers, and ultimately led to better decision-making that balanced technical capabilities with financial constraints and organizational needs. The framework proved particularly effective at helping evaluators look beyond surface-level tool features to examine contextual factors, workflows, and stakeholder needs that determine whether a tool will deliver genuine value in practice.

</details>

The [full case study report still to be added - link not working](to be added) is length TBD 
