# CS3 Tool Design Review: Heuristics Applied to New Feature Design

<details><summary>Click for 1000 word summary generated by Claude Sonnet 4.5</summary>
  
## Method

This case study examined the application of twelve tool evaluation heuristics in the design process for two new features of an existing commercial test tool. The study took place between February and May 2024 at a test tool vendor organisation based in the Netherlands. The organisation's product is a SaaS tool designed specifically for ease of use by non-technical users as well as specialist testers, with an estimated 70% of end users having no IT background. Customers include public authorities, charities, and commercial organisations, with end users spanning medical staff, warehouse operatives, and professional testers.

Three participants were involved: a support lead who acted as key contact and diary author, a lead front-end developer with UX experience who leads design and build of the tool's user experience, and the organisation's director. The heuristics used incorporated improvements identified in earlier case studies.

The case study proceeded through two structured meetings. An initial planning meeting reviewed all twelve heuristics and assessed their relevance to the organisation's context. A subsequent design meeting applied selected heuristics directly to prototypes of two new features: support for exploratory testing, and the ability to tailor role-based workflows. Data was collected via participant diary entries and follow-up discussion with the researcher.

## Results

### Planning Meeting

At the planning meeting, all twelve heuristics were reviewed and an initial response recorded. The overall reaction was that the heuristics appeared theoretical but contained good questions, with the assessment that "the more specific we got, the more useful the heuristics." The participants noted that top-level heuristic questions may be more useful for new tool design or tool selection, while maintenance of an existing tool benefits more from the detailed sub-questions.

Three significant insights emerged at this stage. H01 (Why?) prompted the insight that customers themselves need to be asked why they need the tool, rather than this being assumed - framing the question from the customer's perspective rather than the vendor's. H07 (Where?) led the support lead to ask the front-end developer to consider the physical environments in which end users work, such as operating theatres and warehouse floors, and corresponding design implications including larger buttons and tablet compatibility. H12 (How long?), though judged not relevant to ongoing SaaS design decisions, generated an important insight about the experience of intermittent users: customers who use the tool for a specific project, stop for several months, and return to find the tool has changed significantly through continuous development.

Participants also identified that H04 (Communication needs) was particularly relevant given the breadth of their user base, noting the example of a surgeon testing a medical system between procedures, or a tester whose work is interrupted and completed by a colleague. These situational use cases were not previously explicitly documented in the heuristics explanations, and were added as a result.

### Design Meeting

Seven heuristics were used during the design meeting: H01, H02, H03, H04, H05, H08, and H11. These were applied directly to the prototype features in a time-boxed session attended by all three participants.

H01 and H02 generated the most significant discussions. Working through Why? and Who? together surfaced a critical distinction: the person who purchases the tool (typically a manager or procurement decision-maker) is different from the end user who uses it, and therefore the reasons for needing the tool may differ between these groups. This led to the realisation that the organisation needed to engage more specifically with different roles and personas to understand their distinct needs, particularly business testers who had not been sufficiently represented in existing personas.

H03 (Previous experiences) prompted rich discussion about the diversity of user backgrounds. The distinction between business testers, professional testers, and developers was explored in depth, with the observation that background reflects likely testing knowledge and shapes what support users need. The example of senior medical professionals who are highly capable but unfamiliar with software testing illustrated the importance of not conflating unfamiliarity with testing with lack of capability.

H04, H05, and H06 were discussed together, with participants noting that for a maintenance project where foundational interface and learning decisions have already been made, these heuristics are most useful when focused on specific sub-questions. The question about mastery versus task completion was identified as particularly relevant. Participants also noted that help documentation and instruction videos were currently produced after design rather than informing it, and that new video content aimed at non-professional business testers was in development.

H08 (Workflows) was directly relevant given that workflow tailoring by role was one of the features being designed. H11 (When?) highlighted that the SaaS delivery model - with updates every two weeks - means questions about tool lifespan have limited applicability to design decisions, though the question of tasks split across time and completed by different people was identified as a valuable addition to the heuristics.

H09 (Risks) and H10 (Work styles) were not used, with H09 noted as insufficiently specific for the design context in question.

## Impact and Evaluation

The primary outcome of the design meeting was recognition that the prototype needed to be validated with both customers and end users before implementation. This was expressed directly: "we think we know everything, but that's not the case. You don't know till you ask your customer." This represented a shift from intuitive confidence in existing knowledge to structured acknowledgement of the need for user validation.

The participants, who have high UX maturity and strong intuitive design practice, found that the heuristics formalised and challenged existing thinking rather than introducing entirely new concepts. They noted that many of the heuristics reflect questions they already ask, but that working through them in a structured way prompted consideration of aspects handled unconsciously or incompletely. The sub-questions were consistently more valuable than top-level heuristics in this maintenance context.

A broader insight co-created between participants and researcher was that different stages of a tool's lifecycle require different heuristics. New build scenarios require more extensive use of the full heuristic set, while maintenance contexts require selective use focused on the specific design decisions at hand. Changing a decision made earlier in a product's life requires careful management, and may include consciously deciding not to apply a heuristic.

Improvements identified included developing differentiated routes into the heuristics for new build versus maintenance contexts, adding probing "else?" questions to all heuristics to widen thinking, developing a faster checklist path for organisations with high UX maturity, adding situational examples for occasional and intermittent testers, and providing clearer guidance on when to combine or split heuristics depending on context.

</details>

The full case study report - still to be added - is length TBD 


