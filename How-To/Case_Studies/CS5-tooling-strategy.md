# Case Study 5: Tooling Strategy


<details><summary>Click for 1000 word summary generated by Claude Sonnet 4.5</summary>


## Method

This case study examined the application of twelve tool evaluation heuristics by a consultant planning advice on test automation tooling strategy for an enterprise-level IT organisation. The study took place between June and August 2024. The consultant's organisation provides expert coaching on testing and quality within the testing arena, working with both practitioners and technical leadership.

The context was particularly significant: an enterprise-level organisation with approximately 5,000 development, test, and automation engineers was undertaking a major strategic shift. Testing was moving from being a specialised role performed by approximately 450 test engineers to an activity performed by all 5,000 engineers. This represented a fundamental change from "testing as a role" to "testing as an activity." The tooling team, comprising approximately 40 software development engineers and 8 test automation tooling engineers, develops and maintains six in-house tools based on open-source frameworks, supporting functional test automation, test data tooling, performance testing, test environment tooling, chaos engineering, and contract testing.

The consultant used the heuristics independently to challenge their own thinking while planning this engagement. This was a new context for them, as they had not previously worked as a manager of test automation tooling and had never considered what a tooling strategy should encompass. Data was collected through a detailed mind map the consultant created, documenting their context, thoughts prior to reading the heuristics, their own existing heuristics, their responses to each proposed heuristic, thoughts arising during use, and insights from discussion with the researcher.

## Results

### Prior Thinking and Existing Heuristics

Before reviewing the proposed heuristics, the consultant had already identified several strategic considerations through their experience. These included recognising that new users for the tools were "no longer testers but all engineers," dramatically increasing both the number requiring support and the range of backgrounds and abilities. They recognised that diverse quality improvement techniques beyond test automation were required, including monitoring, alerting, and feature toggling. They proposed repositioning the tooling team's services as an "Engineering Market Place" rather than isolated test automation tooling, noting that software engineers prefer a one-stop shop rather than engaging with multiple separate teams.

The consultant had also considered build versus buy decisions in the context of capital expenditure versus operational expenditure accounting practices, noting that in-house built tools are classified as capital expenditure while purchased tools are operational expenditure, making the cost accounting nature of build more attractive to enterprises despite potentially higher actual costs. They observed that approximately 80% of developer tools are purchased rather than built, while test automation tooling has historically been developed in-house due to testing's complexity.

Significantly, the consultant had independently internalised a set of heuristics substantially aligned with the proposed framework: Who will be using the tool? What context will they work in? How easy is it to pick up and learn? How well does it report? Who is my customer? These mapped to H01, H02, H03, H04, H05, H07, and H08, demonstrating that expert consultants independently arrive at similar questions through experience. The proposed heuristics provided greater detail and made implicit knowledge explicit.

### Assessment of Proposed Heuristics

The consultant categorised the twelve heuristics into three groups. H12 (How long will the tool be used?) and H05 (What learning perspectives or goals?) were "OMG great - I had not thought of this - super useful." H01, H02, H03, H04, H07, and H08 "made me think a little deeper" - easy to answer, able to join the dots, good to think more about areas already partially considered. H06 (Learning preferences), H09 (Risks), H10 (Work styles), and H11 (When?) fell into "not sure what the value is."

The consultant noted they "didn't read a lot of the content," working primarily from top-level heuristic questions, which provided a useful test of how well the questions stand alone without detailed explanations.

### Key Insights by Heuristic

H01 (Why?) was described as context-dependent with reasons depending on the challenge ahead. The consultant noted it as particularly relevant for questioning AI adoption: "I think there's a bias to inject AI into everything. We must go back and ask 'what is the problem we are trying to solve, why is this tool valuable' before going headlong into let's build AI tooling." While engineering thinking typically considers this question, the consultant had identified through workshops that multiple people held multiple goals, and not everyone understood the strategic shift from test engineers to all engineers or from testing as role to testing as activity.

H02 (Who?) was described as "bleeding obvious" yet acknowledged as frequently not interrogated rigorously. During discussion, the consultant spontaneously thought of additional stakeholder groups with different needs, demonstrating that even obvious questions prompt additional useful thinking.

H03 (What previous experiences?) had initially been interpreted as meaning level of experience with the specific tool and possible bias toward or against it, not the broader interpretation in the detailed explanation. This highlighted the importance of clear top-level question wording.

H04 (Communication needs) scored highly and prompted discussion of "in our world" assumptions within engineering communities, and the challenge that engineers prefer working within a single environment but stakeholders have different communication tool preferences, requiring duplication of content across platforms like GitHub and documentation systems.

H05 (Learning perspectives) was valued as helping determine whether the right perspectives were in the room for training decisions. With proportions of 8 test automation engineers to 500 test engineers to 5,000 total engineers, "coach the coach" approaches proved most practical, though questions remained about optimal learning approaches given that self-service training is logistically easiest but may not be optimal for knowledge transfer.

H07 (Where?) prompted thinking about meeting people in their world, noting the same tool might be used differently by different teams, and security requirements varying by environment proximity to production.

H08 (Workflows?) sparked insights about workflow flexibility and quality gates, with the observation that "some people need the gates, some people hate them" depending on individual and organisational maturity. The consultant noted that engineers with low quality understanding benefit from gates, but as they mature and experience incidents, they realise quality's value and push back against rushing.

H12 (How long will the tool be used?) was described as "an excellent question...blew my mind - such a great question." The consultant noted "legacy tooling is a huge cost" yet not a question asked at tools' inception. Key insights included that with thousands of test scripts to retire, "the tool cannot be decommissioned until they are gone - tool not the issue, it's the scripts." The consultant questioned long-term implications of current choices given rapid technology change, and highlighted enormous uncounted costs of maintaining old unsupported systems while teams remain dependent on them. A particularly sharp insight concerned how organisational metrics create perverse incentives: decommissioning 5,000 automated test scripts causes teams to be marked as failing on their key performance indicators for test automation count, even when the scripts provided no value.

H06, H09, H10, and H11 were not found useful in this context. H06 was not initially understood; after discussion the consultant felt it overlapped with H04 and could be merged. H09 was initially not understood in relation to test tools, though discussion surfaced relevant security and scalability risks. H10 required rewording for clarity, leading to its revision to emphasise autonomy. H11 was outside the consultant's control and context-specific.

## Impact and Evaluation

The consultant concluded the heuristics would be valuable in multiple ways: as a learning aid for teams during organisational change when "you don't know what you don't know," for stakeholder conversations to find out what they need because "people go all over the place," and as a framework providing structure easier than free brainstorming. The consultant particularly valued their potential for challenging the current organisational rush to identify AI injection points, refocusing discussions on problems to solve rather than technologies to adopt.

Following the case study, the consultant held a workshop on tooling strategy without explicitly using the heuristics, but reflected that the thinking from analysing them had become embedded intrinsically in their practice, particularly the focus on understanding customer needs through listening tours. Improvements identified included clearer wording, guidance on combining heuristics in specific contexts, and intermediate steps between top-level questions and full explanations.


</details>




The full case study report - still to be added - is length TBD
